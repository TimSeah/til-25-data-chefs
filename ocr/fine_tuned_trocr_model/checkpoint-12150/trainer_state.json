{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 12150,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.024691358024691357,
      "grad_norm": 5.069225311279297,
      "learning_rate": 4.9596707818930046e-05,
      "loss": 6.0236,
      "step": 100
    },
    {
      "epoch": 0.04938271604938271,
      "grad_norm": 8.215603828430176,
      "learning_rate": 4.918518518518519e-05,
      "loss": 4.9636,
      "step": 200
    },
    {
      "epoch": 0.07407407407407407,
      "grad_norm": 5.796509265899658,
      "learning_rate": 4.877366255144033e-05,
      "loss": 4.6965,
      "step": 300
    },
    {
      "epoch": 0.09876543209876543,
      "grad_norm": 6.591036796569824,
      "learning_rate": 4.8362139917695474e-05,
      "loss": 4.459,
      "step": 400
    },
    {
      "epoch": 0.12345679012345678,
      "grad_norm": 7.197348117828369,
      "learning_rate": 4.795061728395062e-05,
      "loss": 4.3093,
      "step": 500
    },
    {
      "epoch": 0.14814814814814814,
      "grad_norm": 3.9256579875946045,
      "learning_rate": 4.754320987654321e-05,
      "loss": 4.1623,
      "step": 600
    },
    {
      "epoch": 0.1728395061728395,
      "grad_norm": 5.151224613189697,
      "learning_rate": 4.7131687242798355e-05,
      "loss": 4.088,
      "step": 700
    },
    {
      "epoch": 0.19753086419753085,
      "grad_norm": 3.4521987438201904,
      "learning_rate": 4.67201646090535e-05,
      "loss": 4.0491,
      "step": 800
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 5.301114559173584,
      "learning_rate": 4.63127572016461e-05,
      "loss": 4.4453,
      "step": 900
    },
    {
      "epoch": 0.24691358024691357,
      "grad_norm": 5.756680011749268,
      "learning_rate": 4.5901234567901235e-05,
      "loss": 4.156,
      "step": 1000
    },
    {
      "epoch": 0.2716049382716049,
      "grad_norm": 4.075037956237793,
      "learning_rate": 4.548971193415638e-05,
      "loss": 4.0512,
      "step": 1100
    },
    {
      "epoch": 0.2962962962962963,
      "grad_norm": 64.44029235839844,
      "learning_rate": 4.5078189300411525e-05,
      "loss": 4.0464,
      "step": 1200
    },
    {
      "epoch": 0.32098765432098764,
      "grad_norm": 13.13589096069336,
      "learning_rate": 4.466666666666667e-05,
      "loss": 3.9771,
      "step": 1300
    },
    {
      "epoch": 0.345679012345679,
      "grad_norm": 9.901023864746094,
      "learning_rate": 4.4255144032921815e-05,
      "loss": 3.8532,
      "step": 1400
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 39.205692291259766,
      "learning_rate": 4.384362139917696e-05,
      "loss": 3.5216,
      "step": 1500
    },
    {
      "epoch": 0.3950617283950617,
      "grad_norm": 9.754850387573242,
      "learning_rate": 4.34320987654321e-05,
      "loss": 3.4588,
      "step": 1600
    },
    {
      "epoch": 0.41975308641975306,
      "grad_norm": 32.3317985534668,
      "learning_rate": 4.3020576131687244e-05,
      "loss": 3.3956,
      "step": 1700
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 6.690258026123047,
      "learning_rate": 4.260905349794239e-05,
      "loss": 3.3206,
      "step": 1800
    },
    {
      "epoch": 0.4691358024691358,
      "grad_norm": 49.53693771362305,
      "learning_rate": 4.2197530864197534e-05,
      "loss": 3.2573,
      "step": 1900
    },
    {
      "epoch": 0.49382716049382713,
      "grad_norm": 39.71868896484375,
      "learning_rate": 4.178600823045268e-05,
      "loss": 3.0695,
      "step": 2000
    },
    {
      "epoch": 0.5185185185185185,
      "grad_norm": 25.410982131958008,
      "learning_rate": 4.137448559670782e-05,
      "loss": 3.2451,
      "step": 2100
    },
    {
      "epoch": 0.5432098765432098,
      "grad_norm": 22.36012077331543,
      "learning_rate": 4.096296296296296e-05,
      "loss": 3.1953,
      "step": 2200
    },
    {
      "epoch": 0.5679012345679012,
      "grad_norm": 11.327731132507324,
      "learning_rate": 4.055144032921811e-05,
      "loss": 2.9556,
      "step": 2300
    },
    {
      "epoch": 0.5925925925925926,
      "grad_norm": 28.96842384338379,
      "learning_rate": 4.0144032921810705e-05,
      "loss": 3.0008,
      "step": 2400
    },
    {
      "epoch": 0.6172839506172839,
      "grad_norm": 35.08857345581055,
      "learning_rate": 3.973251028806584e-05,
      "loss": 3.0446,
      "step": 2500
    },
    {
      "epoch": 0.6419753086419753,
      "grad_norm": 56.229278564453125,
      "learning_rate": 3.932098765432099e-05,
      "loss": 3.1439,
      "step": 2600
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 53.25995635986328,
      "learning_rate": 3.890946502057613e-05,
      "loss": 2.975,
      "step": 2700
    },
    {
      "epoch": 0.691358024691358,
      "grad_norm": 82.5238265991211,
      "learning_rate": 3.849794238683128e-05,
      "loss": 3.3493,
      "step": 2800
    },
    {
      "epoch": 0.7160493827160493,
      "grad_norm": 69.40940856933594,
      "learning_rate": 3.808641975308642e-05,
      "loss": 2.9087,
      "step": 2900
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 34.31744384765625,
      "learning_rate": 3.767489711934157e-05,
      "loss": 3.2684,
      "step": 3000
    },
    {
      "epoch": 0.7654320987654321,
      "grad_norm": 9.69774055480957,
      "learning_rate": 3.7263374485596706e-05,
      "loss": 3.4022,
      "step": 3100
    },
    {
      "epoch": 0.7901234567901234,
      "grad_norm": 7.095362186431885,
      "learning_rate": 3.685185185185185e-05,
      "loss": 3.1446,
      "step": 3200
    },
    {
      "epoch": 0.8148148148148148,
      "grad_norm": 7.6301140785217285,
      "learning_rate": 3.6440329218107e-05,
      "loss": 2.8627,
      "step": 3300
    },
    {
      "epoch": 0.8395061728395061,
      "grad_norm": 49.937618255615234,
      "learning_rate": 3.602880658436214e-05,
      "loss": 2.8247,
      "step": 3400
    },
    {
      "epoch": 0.8641975308641975,
      "grad_norm": 13.268145561218262,
      "learning_rate": 3.5617283950617286e-05,
      "loss": 2.864,
      "step": 3500
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 62.94192123413086,
      "learning_rate": 3.520576131687243e-05,
      "loss": 2.7743,
      "step": 3600
    },
    {
      "epoch": 0.9135802469135802,
      "grad_norm": 100.95450592041016,
      "learning_rate": 3.479423868312757e-05,
      "loss": 2.9379,
      "step": 3700
    },
    {
      "epoch": 0.9382716049382716,
      "grad_norm": 8.66817569732666,
      "learning_rate": 3.438271604938272e-05,
      "loss": 2.6997,
      "step": 3800
    },
    {
      "epoch": 0.9629629629629629,
      "grad_norm": 59.632347106933594,
      "learning_rate": 3.3971193415637866e-05,
      "loss": 2.7799,
      "step": 3900
    },
    {
      "epoch": 0.9876543209876543,
      "grad_norm": 59.48535919189453,
      "learning_rate": 3.3559670781893004e-05,
      "loss": 2.6409,
      "step": 4000
    },
    {
      "epoch": 1.0123456790123457,
      "grad_norm": 34.91680145263672,
      "learning_rate": 3.314814814814815e-05,
      "loss": 2.8137,
      "step": 4100
    },
    {
      "epoch": 1.037037037037037,
      "grad_norm": 48.667842864990234,
      "learning_rate": 3.2736625514403295e-05,
      "loss": 2.6372,
      "step": 4200
    },
    {
      "epoch": 1.0617283950617284,
      "grad_norm": 42.87459945678711,
      "learning_rate": 3.232510288065844e-05,
      "loss": 2.8033,
      "step": 4300
    },
    {
      "epoch": 1.0864197530864197,
      "grad_norm": 15.283483505249023,
      "learning_rate": 3.1913580246913585e-05,
      "loss": 2.8209,
      "step": 4400
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 22.360210418701172,
      "learning_rate": 3.150205761316873e-05,
      "loss": 2.6591,
      "step": 4500
    },
    {
      "epoch": 1.1358024691358024,
      "grad_norm": 94.38825225830078,
      "learning_rate": 3.109053497942387e-05,
      "loss": 2.903,
      "step": 4600
    },
    {
      "epoch": 1.1604938271604939,
      "grad_norm": 45.215232849121094,
      "learning_rate": 3.067901234567901e-05,
      "loss": 2.9132,
      "step": 4700
    },
    {
      "epoch": 1.1851851851851851,
      "grad_norm": 13.738358497619629,
      "learning_rate": 3.026748971193416e-05,
      "loss": 2.7929,
      "step": 4800
    },
    {
      "epoch": 1.2098765432098766,
      "grad_norm": 8.123631477355957,
      "learning_rate": 2.98559670781893e-05,
      "loss": 2.6188,
      "step": 4900
    },
    {
      "epoch": 1.2345679012345678,
      "grad_norm": 88.13299560546875,
      "learning_rate": 2.9444444444444448e-05,
      "loss": 2.7422,
      "step": 5000
    },
    {
      "epoch": 1.2592592592592593,
      "grad_norm": 108.7093734741211,
      "learning_rate": 2.9032921810699593e-05,
      "loss": 2.7421,
      "step": 5100
    },
    {
      "epoch": 1.2839506172839505,
      "grad_norm": 41.36219024658203,
      "learning_rate": 2.8621399176954735e-05,
      "loss": 2.5632,
      "step": 5200
    },
    {
      "epoch": 1.308641975308642,
      "grad_norm": 33.1865348815918,
      "learning_rate": 2.820987654320988e-05,
      "loss": 2.4443,
      "step": 5300
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 107.5868148803711,
      "learning_rate": 2.7798353909465018e-05,
      "loss": 2.4862,
      "step": 5400
    },
    {
      "epoch": 1.3580246913580247,
      "grad_norm": 37.71312713623047,
      "learning_rate": 2.7386831275720166e-05,
      "loss": 2.5654,
      "step": 5500
    },
    {
      "epoch": 1.382716049382716,
      "grad_norm": 82.79203796386719,
      "learning_rate": 2.697530864197531e-05,
      "loss": 2.6787,
      "step": 5600
    },
    {
      "epoch": 1.4074074074074074,
      "grad_norm": 82.00090026855469,
      "learning_rate": 2.6563786008230453e-05,
      "loss": 2.3981,
      "step": 5700
    },
    {
      "epoch": 1.4320987654320987,
      "grad_norm": 100.25379180908203,
      "learning_rate": 2.6152263374485598e-05,
      "loss": 2.5601,
      "step": 5800
    },
    {
      "epoch": 1.4567901234567902,
      "grad_norm": 7.605125904083252,
      "learning_rate": 2.5740740740740743e-05,
      "loss": 2.5196,
      "step": 5900
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 97.64344024658203,
      "learning_rate": 2.5329218106995885e-05,
      "loss": 2.4321,
      "step": 6000
    },
    {
      "epoch": 1.5061728395061729,
      "grad_norm": 57.43865203857422,
      "learning_rate": 2.491769547325103e-05,
      "loss": 2.5721,
      "step": 6100
    },
    {
      "epoch": 1.5308641975308643,
      "grad_norm": 156.05752563476562,
      "learning_rate": 2.4506172839506175e-05,
      "loss": 2.5316,
      "step": 6200
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 16.62350845336914,
      "learning_rate": 2.409465020576132e-05,
      "loss": 2.5845,
      "step": 6300
    },
    {
      "epoch": 1.5802469135802468,
      "grad_norm": 38.93795394897461,
      "learning_rate": 2.368312757201646e-05,
      "loss": 2.3306,
      "step": 6400
    },
    {
      "epoch": 1.6049382716049383,
      "grad_norm": 19.349597930908203,
      "learning_rate": 2.3275720164609055e-05,
      "loss": 2.2684,
      "step": 6500
    },
    {
      "epoch": 1.6296296296296298,
      "grad_norm": 42.877681732177734,
      "learning_rate": 2.2868312757201646e-05,
      "loss": 2.5545,
      "step": 6600
    },
    {
      "epoch": 1.654320987654321,
      "grad_norm": 56.951473236083984,
      "learning_rate": 2.245679012345679e-05,
      "loss": 2.3784,
      "step": 6700
    },
    {
      "epoch": 1.6790123456790123,
      "grad_norm": 77.37401580810547,
      "learning_rate": 2.2045267489711936e-05,
      "loss": 2.3678,
      "step": 6800
    },
    {
      "epoch": 1.7037037037037037,
      "grad_norm": 74.10826110839844,
      "learning_rate": 2.1633744855967078e-05,
      "loss": 3.7012,
      "step": 6900
    },
    {
      "epoch": 1.7283950617283952,
      "grad_norm": 10.086012840270996,
      "learning_rate": 2.1222222222222223e-05,
      "loss": 2.7692,
      "step": 7000
    },
    {
      "epoch": 1.7530864197530864,
      "grad_norm": 106.87312316894531,
      "learning_rate": 2.0810699588477368e-05,
      "loss": 2.4336,
      "step": 7100
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 84.0543212890625,
      "learning_rate": 2.039917695473251e-05,
      "loss": 2.275,
      "step": 7200
    },
    {
      "epoch": 1.8024691358024691,
      "grad_norm": 82.75568389892578,
      "learning_rate": 1.9987654320987654e-05,
      "loss": 2.35,
      "step": 7300
    },
    {
      "epoch": 1.8271604938271606,
      "grad_norm": 89.6897201538086,
      "learning_rate": 1.95761316872428e-05,
      "loss": 2.3698,
      "step": 7400
    },
    {
      "epoch": 1.8518518518518519,
      "grad_norm": 96.55049133300781,
      "learning_rate": 1.9168724279835393e-05,
      "loss": 2.2196,
      "step": 7500
    },
    {
      "epoch": 1.876543209876543,
      "grad_norm": 59.16789245605469,
      "learning_rate": 1.8757201646090535e-05,
      "loss": 2.264,
      "step": 7600
    },
    {
      "epoch": 1.9012345679012346,
      "grad_norm": 71.6963882446289,
      "learning_rate": 1.834567901234568e-05,
      "loss": 2.2196,
      "step": 7700
    },
    {
      "epoch": 1.925925925925926,
      "grad_norm": 69.66725158691406,
      "learning_rate": 1.7934156378600825e-05,
      "loss": 2.3678,
      "step": 7800
    },
    {
      "epoch": 1.9506172839506173,
      "grad_norm": 6.809170722961426,
      "learning_rate": 1.7522633744855967e-05,
      "loss": 2.3468,
      "step": 7900
    },
    {
      "epoch": 1.9753086419753085,
      "grad_norm": 124.03367614746094,
      "learning_rate": 1.7111111111111112e-05,
      "loss": 2.2025,
      "step": 8000
    },
    {
      "epoch": 2.0,
      "grad_norm": 65.29061126708984,
      "learning_rate": 1.6699588477366257e-05,
      "loss": 2.1413,
      "step": 8100
    },
    {
      "epoch": 2.0246913580246915,
      "grad_norm": 135.78500366210938,
      "learning_rate": 1.62880658436214e-05,
      "loss": 2.2331,
      "step": 8200
    },
    {
      "epoch": 2.049382716049383,
      "grad_norm": 87.57870483398438,
      "learning_rate": 1.5876543209876543e-05,
      "loss": 2.1805,
      "step": 8300
    },
    {
      "epoch": 2.074074074074074,
      "grad_norm": 10.560589790344238,
      "learning_rate": 1.546502057613169e-05,
      "loss": 2.1975,
      "step": 8400
    },
    {
      "epoch": 2.0987654320987654,
      "grad_norm": 25.184045791625977,
      "learning_rate": 1.5053497942386832e-05,
      "loss": 2.1278,
      "step": 8500
    },
    {
      "epoch": 2.123456790123457,
      "grad_norm": 47.036033630371094,
      "learning_rate": 1.4641975308641975e-05,
      "loss": 2.0317,
      "step": 8600
    },
    {
      "epoch": 2.148148148148148,
      "grad_norm": 90.34139251708984,
      "learning_rate": 1.4230452674897122e-05,
      "loss": 2.165,
      "step": 8700
    },
    {
      "epoch": 2.1728395061728394,
      "grad_norm": 167.513671875,
      "learning_rate": 1.3818930041152263e-05,
      "loss": 2.1203,
      "step": 8800
    },
    {
      "epoch": 2.197530864197531,
      "grad_norm": 60.015357971191406,
      "learning_rate": 1.3407407407407407e-05,
      "loss": 2.33,
      "step": 8900
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 19.650300979614258,
      "learning_rate": 1.2995884773662553e-05,
      "loss": 2.1062,
      "step": 9000
    },
    {
      "epoch": 2.246913580246914,
      "grad_norm": 9.33485221862793,
      "learning_rate": 1.2584362139917697e-05,
      "loss": 2.0932,
      "step": 9100
    },
    {
      "epoch": 2.271604938271605,
      "grad_norm": 96.24205780029297,
      "learning_rate": 1.217283950617284e-05,
      "loss": 2.1514,
      "step": 9200
    },
    {
      "epoch": 2.2962962962962963,
      "grad_norm": 116.9902114868164,
      "learning_rate": 1.1761316872427983e-05,
      "loss": 2.1645,
      "step": 9300
    },
    {
      "epoch": 2.3209876543209877,
      "grad_norm": 85.49018859863281,
      "learning_rate": 1.1349794238683128e-05,
      "loss": 2.1128,
      "step": 9400
    },
    {
      "epoch": 2.3456790123456788,
      "grad_norm": 55.29776382446289,
      "learning_rate": 1.0938271604938272e-05,
      "loss": 2.04,
      "step": 9500
    },
    {
      "epoch": 2.3703703703703702,
      "grad_norm": 59.44927978515625,
      "learning_rate": 1.0526748971193417e-05,
      "loss": 2.0118,
      "step": 9600
    },
    {
      "epoch": 2.3950617283950617,
      "grad_norm": 93.6607666015625,
      "learning_rate": 1.011522633744856e-05,
      "loss": 1.9189,
      "step": 9700
    },
    {
      "epoch": 2.419753086419753,
      "grad_norm": 192.62347412109375,
      "learning_rate": 9.703703703703703e-06,
      "loss": 1.9563,
      "step": 9800
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 87.47782135009766,
      "learning_rate": 9.292181069958848e-06,
      "loss": 1.9635,
      "step": 9900
    },
    {
      "epoch": 2.4691358024691357,
      "grad_norm": 126.07064056396484,
      "learning_rate": 8.880658436213994e-06,
      "loss": 1.9353,
      "step": 10000
    },
    {
      "epoch": 2.493827160493827,
      "grad_norm": 9.566139221191406,
      "learning_rate": 8.469135802469135e-06,
      "loss": 2.0756,
      "step": 10100
    },
    {
      "epoch": 2.5185185185185186,
      "grad_norm": 13.076630592346191,
      "learning_rate": 8.05761316872428e-06,
      "loss": 1.887,
      "step": 10200
    },
    {
      "epoch": 2.5432098765432096,
      "grad_norm": 82.75194549560547,
      "learning_rate": 7.646090534979425e-06,
      "loss": 1.9038,
      "step": 10300
    },
    {
      "epoch": 2.567901234567901,
      "grad_norm": 137.26930236816406,
      "learning_rate": 7.234567901234568e-06,
      "loss": 1.944,
      "step": 10400
    },
    {
      "epoch": 2.5925925925925926,
      "grad_norm": 9.909762382507324,
      "learning_rate": 6.823045267489713e-06,
      "loss": 1.8723,
      "step": 10500
    },
    {
      "epoch": 2.617283950617284,
      "grad_norm": 45.425209045410156,
      "learning_rate": 6.411522633744857e-06,
      "loss": 1.864,
      "step": 10600
    },
    {
      "epoch": 2.6419753086419755,
      "grad_norm": 28.18880271911621,
      "learning_rate": 6e-06,
      "loss": 1.8465,
      "step": 10700
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 19.660930633544922,
      "learning_rate": 5.588477366255144e-06,
      "loss": 1.7429,
      "step": 10800
    },
    {
      "epoch": 2.691358024691358,
      "grad_norm": 61.79673767089844,
      "learning_rate": 5.1769547325102885e-06,
      "loss": 1.852,
      "step": 10900
    },
    {
      "epoch": 2.7160493827160495,
      "grad_norm": 258.3022766113281,
      "learning_rate": 4.765432098765432e-06,
      "loss": 1.8407,
      "step": 11000
    },
    {
      "epoch": 2.7407407407407405,
      "grad_norm": 116.72991180419922,
      "learning_rate": 4.353909465020577e-06,
      "loss": 1.7947,
      "step": 11100
    },
    {
      "epoch": 2.765432098765432,
      "grad_norm": 86.76543426513672,
      "learning_rate": 3.94238683127572e-06,
      "loss": 1.8078,
      "step": 11200
    },
    {
      "epoch": 2.7901234567901234,
      "grad_norm": 38.23604965209961,
      "learning_rate": 3.5308641975308643e-06,
      "loss": 1.8543,
      "step": 11300
    },
    {
      "epoch": 2.814814814814815,
      "grad_norm": 29.74367904663086,
      "learning_rate": 3.1193415637860085e-06,
      "loss": 1.7225,
      "step": 11400
    },
    {
      "epoch": 2.8395061728395063,
      "grad_norm": 27.784706115722656,
      "learning_rate": 2.7078189300411522e-06,
      "loss": 1.762,
      "step": 11500
    },
    {
      "epoch": 2.8641975308641974,
      "grad_norm": 78.57415008544922,
      "learning_rate": 2.2962962962962964e-06,
      "loss": 1.7866,
      "step": 11600
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 38.9131965637207,
      "learning_rate": 1.8847736625514406e-06,
      "loss": 1.6606,
      "step": 11700
    },
    {
      "epoch": 2.9135802469135803,
      "grad_norm": 11.669434547424316,
      "learning_rate": 1.4732510288065843e-06,
      "loss": 1.7083,
      "step": 11800
    },
    {
      "epoch": 2.9382716049382713,
      "grad_norm": 21.373821258544922,
      "learning_rate": 1.0617283950617285e-06,
      "loss": 1.6774,
      "step": 11900
    },
    {
      "epoch": 2.962962962962963,
      "grad_norm": 34.010276794433594,
      "learning_rate": 6.502057613168725e-07,
      "loss": 1.6839,
      "step": 12000
    },
    {
      "epoch": 2.9876543209876543,
      "grad_norm": 134.16717529296875,
      "learning_rate": 2.3868312757201647e-07,
      "loss": 1.804,
      "step": 12100
    }
  ],
  "logging_steps": 100,
  "max_steps": 12150,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 9.091665070601011e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
