# 🚀 TIL-25 Hackathon - Data Chefs

<!-- TOC -->
* [🚀 TIL-25 Hackathon - Data Chefs](#-til-25-hackathon---data-chefs)
* [🧑‍🍳 Our Team: Data Chefs](#-our-team-data-chefs)
* [🏆 Our Achievements](#-our-achievements)
* [📊 Final Evaluation Results](#-final-evaluation-results)
* [🧠 DSTA BrainHack TIL-AI 2025: Competition Overview & Resources](#-dsta-brainhack-til-ai-2025-competition-overview--resources)
  * [🏗️ Understanding the Competition Repo Structure](#️-understanding-the-competition-repo-structure)
  * [🔗 Key Competition Links](#️-key-competition-links)
* [🎯 Purpose of Our Repository (til-25-data-chefs)](#-purpose-of-our-repository-til-25-data-chefs)
* [🛠️ Our Setup and Usage](#️-our-setup-and-usage)
  * [✅ Prerequisites (During Hackathon)](#️-prerequisites-during-hackathon)
  * [⚙️ How We Set Up This Repo (Initial Hackathon Setup)](#️-how-we-set-up-this-repo-initial-hackathon-setup)
  * [🔄 Our Development Workflow (During Hackathon)](#️-our-development-workflow-during-hackathon)
    * [💻 Working on a Challenge](#-working-on-a-challenge)
    * [📦 Building, Testing, and Submitting](#-building-testing-and-submitting)
* [📁 Our File Structure](#-our-file-structure)
* [📄 Optical Character Recognition (OCR) Challenge](#-optical-character-recognition-ocr-challenge)
  * [📝 Description](#-description)
  * [🔗 Our OCR Repository Link](#️-our-ocr-repository-link)
  * [💻 Key Technologies We Used](#-key-technologies-we-used)
  * [✨ Our Solution & Key Achievements](#️-our-solution--key-achievements)
* [🤖 Reinforcement Learning (RL) Challenge](#-reinforcement-learning-rl-challenge)
  * [📝 Description](#-description-1)
  * [🔗 Our RL Repository Link](#️-our-rl-repository-link)
  * [💻 Key Technologies We Used](#-key-technologies-we-used-1)
  * [✨ Our Solution & Key Achievements](#️-our-solution--key-achievements-1)
* [🖼️ Computer Vision (CV) Challenge](#️-computer-vision-cv-challenge)
  * [📝 Description](#-description-2)
  * [🔗 Our CV Repository Link](#️-our-cv-repository-link)
  * [💻 Key Technologies We Used](#-key-technologies-we-used-2)
  * [✨ Our Solution & Key Achievements](#️-our-solution--key-achievements-2)
* [🎤 Automatic Speech Recognition (ASR) Challenge](#-automatic-speech-recognition-asr-challenge)
  * [📝 Description](#-description-3)
  * [🔗 Our ASR Repository Link](#️-our-asr-repository-link)
  * [💻 Key Technologies We Used](#-key-technologies-we-used-3)
  * [✨ Our Solution & Key Achievements](#️-our-solution--key-achievements-3)
* [🧩 Surprise Task: Document Reassembly](#-surprise-task-document-reassembly)
  * [📝 Description](#-description-4)
  * [🔗 Our Surprise Task Repository Link](#️-our-surprise-task-repository-link)
  * [💻 Key Technologies We Used](#-key-technologies-we-used-4)
  * [✨ Our Solution & Key Achievements](#️-our-solution--key-achievements-4)
* [🎉 Final Words from Data Chefs](#-final-words-from-data-chefs)
<!-- TOC -->

## Introduction

Welcome to the main repository for our team, "Data Chefs," from the TIL-25 Hackathon! This repository served as our central hub for the project.

**The Narrative:** CYPHER, the AI-Droid villain, deployed autonomous bogies closing in on Singapore. DSTA tasked all participating teams, including ours, to wargame a counter-offensive. Our droid had to autonomously recon unknown terrain, gather intel, analyze the ground situation, and report findings, all while avoiding CYPHER's cronies.

**Our Mission:**
*   Infiltrate CYPHER's base
*   Obtain valuable intelligence
*   Don't get captured!

As Data Chefs, we tackled the following challenges:
*   **Automatic Speech Recognition (ASR):** Converting spoken audio into accurate text.
    *   [TimSeah/til-25-data-chefs-ASR](https://github.com/TimSeah/til-25-data-chefs-ASR)
*   **Computer Vision (CV):** Detecting, locating, and classifying objects in images.
    *   [TimSeah/til-25-data-chefs-CV](https://github.com/TimSeah/til-25-data-chefs-CV)
*   **Optical Character Recognition (OCR):** Identifying and extracting text from documents.
    *   [TimSeah/til-25-data-chefs-OCR](https://github.com/TimSeah/til-25-data-chefs-OCR)
*   **Reinforcement Learning (RL):** Developing algorithms for autonomous robot navigation.
    *   [TimSeah/til-25-data-chefs-RL](https://github.com/TimSeah/til-25-data-chefs-RL)
*   **Surprise Task:** Reconstructing fragments of shredded documents.
    *   [TimSeah/til-25-data-chefs-surprise](https://github.com/TimSeah/til-25-data-chefs-surprise)

## 🧑‍🍳 Our Team: Data Chefs
*   [Timothy Seah](https://github.com/TimSeah)
*   [Darren Goh](https://github.com/dgxy2002)
*   [Felix Teo](https://github.com/felix21g)
*   [Freddie Loh](https://github.com/FredSterz)

## 🏆 Our Achievements
*   9th in Qualifiers
*   3rd in Semi-Finals
*   5th in Finals

## 📊 Final Evaluation Results

| Task          | Our Score | Our Speed |
|---------------|-----------|-----------|
| ASR           | 0.915     | 0.779     |
| CV            | 0.256     | 0.907     |
| OCR           | 0.591     | 0.820     |
| RL            | 0.740     | 0.994     |
| Surprise Task | 0.830     | 0.970     |
| Overall       | 0.711     |   -       |

## 🧠 DSTA BrainHack TIL-AI 2025: Competition Overview & Resources

Our project was developed as part of the DSTA BrainHack TIL-AI 2025 competition. The competition provided a template repository (`til-ai/til-25`) which this repository, `TimSeah/til-25-data-chefs`, is based on.

![Banner for TIL-AI](https://static.wixstatic.com/media/b03c31_bdb8962d37364d7c8cc3e6ae234bb172~mv2.png/v1/crop/x_0,y_1,w_3392,h_1453/fill/w_3310,h_1418,al_c,q_95,usm_0.66_1.00_0.01,enc_avif,quality_auto/Brainhack%20KV_v12_FOR_WEB.png)

### 🏗️ Understanding the Competition Repo Structure
The competition was structured with a main repository containing subdirectories for each challenge: `asr/`, `cv/`, `ocr/`, `rl/`, and later `surprise/`. Each of these directories was expected to contain:
*   A `src/` directory for the core model code (`*_manager.py`) and server logic (`*_server.py`).
*   A `Dockerfile` for building the model's Docker image.
*   A `requirements.txt` for Python dependencies.
*   A `README.md` with challenge-specific specifications.

A `test/` directory was provided with tools for local testing and scoring.

Two crucial Git submodules were part of the setup:
*   `til-25-finals`: Contained code pulled into our repo for Semifinals and Finals.
*   `til-25-environment`: Contained the `til_environment` package, essential for training and testing our RL model.

### 🔗 Key Competition Links
These resources were vital to us during the hackathon:
*   **Competition Wiki:** For tutorials, specifications, and more. (Found at `https://github.com/til-ai/til-25/wiki`)
*   **Vertex AI Workbench:** Our primary development environment on Google Cloud Platform.
*   **Guardian's Handbook:** Housed the Leaderboard and general competition information.
*   **TIL-AI Curriculum:** Contained educational materials for the competition.
*   **Discord (#hackoverflow channel):** The forum for participants.

## 🎯 Purpose of Our Repository (til-25-data-chefs)

This repository, `TimSeah/til-25-data-chefs`, was the central working space for our team, "Data Chefs," during the DSTA BrainHack TIL-AI 2025. We created it based on the official `til-ai/til-25` competition template repository.

It housed all the code, models, and documentation we developed for the challenges. While we managed detailed model development for each challenge in separate dedicated repositories (linked below), this repository served as our primary fork from the competition template. It contained the necessary structure for the competition, submission scripts, and our overall team documentation.

## 🛠️ Our Setup and Usage

This section outlines the setup and usage patterns we followed during the hackathon, based on the DSTA BrainHack TIL-AI 2025 guidelines.

### ✅ Prerequisites (During Hackathon)
*   Python 3.12 or newer
*   Conda (for environment management)
*   Docker (for building and running models)
*   Git (for version control)
*   GitHub Personal Access Token (for cloning private template repositories)

### ⚙️ How We Set Up This Repo (Initial Hackathon Setup)
1.  **Template Creation:** We created this repository using the official `til-ai/til-25` repository as a template.
2.  **Cloning:** We cloned our repository into our Vertex AI workbenches.
    ```bash
    git clone https://github.com/TimSeah/til-25-data-chefs.git
    cd til-25-data-chefs
    ```
3.  **Initialize Git Submodules:** We ran the following to pull in `til-25-finals` and `til-25-environment`:
    ```bash
    git submodule update --init
    ```
4.  **Create Conda Environment:** We set up our Python environment using Conda:
    ```bash
    conda create --name til python=3.12
    conda activate til
    ```
5.  **Install Development Dependencies:** We installed the necessary development packages:
    ```bash
    pip install -r requirements-dev.txt
    ```

### 🔄 Our Development Workflow (During Hackathon)

#### 💻 Working on a Challenge
For each challenge (ASR, CV, OCR, RL, Surprise), our process was:
1.  Navigate to the respective challenge subdirectory (e.g., `cd ./ocr/`).
2.  Develop the core logic in `src/*_manager.py`.
3.  Implement server interactions in `src/*_server.py`.
4.  List Python package dependencies in `requirements.txt`.
5.  Configure the `Dockerfile` for building the challenge-specific Docker image.

We conducted intensive model development and experimentation for each challenge in their dedicated repositories:
*   OCR: [TimSeah/til-25-data-chefs-OCR](https://github.com/TimSeah/til-25-data-chefs-OCR)
*   RL: [TimSeah/til-25-data-chefs-RL](https://github.com/TimSeah/til-25-data-chefs-RL)
*   CV: [TimSeah/til-25-data-chefs-CV](https://github.com/TimSeah/til-25-data-chefs-CV)
*   ASR: [TimSeah/til-25-data-chefs-ASR](https://github.com/TimSeah/til-25-data-chefs-ASR)
*   Surprise: [TimSeah/til-25-data-chefs-surprise](https://github.com/TimSeah/til-25-data-chefs-surprise)

We then integrated the finalized code from these specialized repositories into the corresponding challenge subdirectories within this main `til-25-data-chefs` repository for building and submission.

#### 📦 Building, Testing, and Submitting
Our submission process for each challenge was:
1.  **Navigate to Challenge Directory:**
    ```bash
    # Example for ASR
    cd ./asr/
    # For Surprise Task
    # cd ./surprise/
    ```
2.  **Build Docker Image:**
    ```bash
    # Example for ASR
    docker build -t data-chefs-asr:latest .
    # Example for Surprise Task
    # docker build -t data-chefs-surprise:latest .
    ```
3.  **Local Testing (Crucial Step!):**
    *   Run the Docker container:
        ```bash
        # Example for ASR (port 8080 typical for ASR/CV/OCR/RL)
        docker run -p 8080 --gpus all -d data-chefs-asr:latest
        # Example for Surprise Task (port 5005 specified)
        # docker run -p 5005:5005 --gpus all -d data-chefs-surprise:latest
        ```
    *   Run the local test script:
        ```bash
        # General challenges
        python ../test/test_asr.py # Test scripts were in the parent test/ directory
        # For Surprise Task (assuming test.py is in the root of surprise task repo or subdir)
        # python3 test.py
        ```
4.  **Submit for Evaluation:**
    ```bash
    # General challenges
    til submit data-chefs-asr:latest
    # For Surprise Task (using the provided submit script)
    # ./submit-surprise.sh data-chefs-surprise:latest
    ```

## 📁 Our File Structure

The structure of this repository, as used during the competition, mirrored the `til-ai/til-25` template, with an additional `surprise/` directory:

```
til-25-data-chefs/
├── asr/                        # Automatic Speech Recognition Challenge
│   ├── src/ ...
├── cv/                         # Computer Vision Challenge
│   ├── src/ ...
├── ocr/                        # Optical Character Recognition Challenge
│   ├── src/ ...
├── rl/                         # Reinforcement Learning Challenge
│   ├── src/ ...
├── surprise/                   # Surprise Task: Document Reassembly
│   ├── src/
│   │   ├── surprise_manager.py
│   │   └── surprise_server.py
│   ├── Dockerfile
│   ├── requirements.txt
│   └── README.md               # Specific to surprise task
├── test/                       # Local testing tools and scripts
│   ├── test_asr.py
│   ├── test_cv.py
│   ├── test_ocr.py
│   ├── test_rl.py
│   └── test_surprise.py        # (or similar, following convention)
├── til-25-environment/         # Git submodule for RL environment
├── til-25-finals/              # Git submodule for Semifinals/Finals code
├── .gitmodules                 # Git submodule configuration
├── requirements-dev.txt        # Development dependencies for the root environment
└── README.md                   # This file, our team's main documentation!
```

## 📄 Optical Character Recognition (OCR) Challenge
### 📝 Description
The challenge was to identify and extract text from various documents.

### 🔗 Our OCR Repository Link
[TimSeah/til-25-data-chefs-OCR](https://github.com/TimSeah/til-25-data-chefs-OCR) – This is where we did the heavy lifting for OCR model development.

### 💻 Key Technologies We Used
*   **PaddleOCR (v2.10.0):** This was our core OCR engine for text detection, recognition, and layout analysis.
*   **Python:** Our primary language for scripting, model training, and inference.
*   **Jupyter Notebooks:** We used these extensively for experimentation and model development.
*   **Shell Scripts:** For automating tasks, managing datasets, and running pipelines.
*   **OpenCV (`opencv-python-headless`):** Leveraged for image processing tasks.
*   **NumPy:** For efficient numerical computations with image data.
*   **Docker:** For creating consistent environments for dataset construction and model deployment.
*   **FastAPI & Uvicorn:** We integrated these for serving our OCR model as an API.

### ✨ Our Solution & Key Achievements
We utilized PaddleOCR, employing a multi-stage pipeline:

1.  **Layout Analysis:** We used the `picodet_lcnet_x1_0_fgd_layout_infer` model for understanding document structure.
2.  **Text Detection:** The `en_PP-OCRv3_det_infer` model helped us accurately locate text regions.
3.  **Text Direction Classification:** `ch_ppocr_mobile_v2.0_cls_infer` determined text orientation.
4.  **Text Recognition:** Initially, we used `en_PP-OCRv4_rec_infer`.

**Finetuning for Enhanced Performance:**
To boost accuracy, we finetuned the `en_PP-OCRv4_rec_train` recognition model. Key aspects of our finetuning:
*   **Custom Character Set:** We created a `custom_char_dict.txt` specific to the dataset.
*   **Dataset:** Used `rec_gt_train.txt` for training and `rec_gt_eval.txt` for evaluation. Max text length was 128.
*   **Training Parameters:** 100 epochs, Adam optimizer, Cosine learning rate (initial LR 0.0001, 2-epoch warmup), batch size 64/GPU.
*   **Architecture:** SVTR (Vision Transformer) with a MobileNetV1Enhance backbone and CTCHead/CTCLoss.
*   **Preprocessing:** Images resized to `[3, 48, 320]` and normalized.
*   **Starting Point:** Finetuned from `best_accuracy` of `en_PP-OCRv4_rec_train`.

This finetuning was vital for adapting the model to the competition data.

## 🤖 Reinforcement Learning (RL) Challenge
### 📝 Description
We developed algorithms for a simulated robot to autonomously navigate an environment through trial-and-error learning. The environment involved a maze layout where teams played as "Scout" or "Guard" in 4 total rounds, round-robin style.

### 🔗 Our RL Repository Link
[TimSeah/til-25-data-chefs-RL](https://github.com/TimSeah/til-25-data-chefs-RL)

### 💻 Key Technologies We Used
*   **Python:** Primary programming language for implementing the RL algorithms.
*   **Jupyter Notebooks:** Used extensively for model development, training, and evaluation.
*   **PyTorch:** The deep learning framework used for building and training the neural networks.
*   **Deep Q-Network (DQN):** The core RL algorithm. The Q-function was approximated using a Multi-Layer Perceptron (MLP) processing engineered visual and state features.
*   **Prioritized Experience Replay (PER):** Implemented using a SumTree to sample more important transitions more frequently.
*   **Epsilon-Greedy Exploration:** Used for action selection during training, with a decaying epsilon value.
*   **Target Network & Double DQN:** Utilized to stabilize training and reduce Q-value overestimation.
*   **Adam Optimizer:** Used for training the neural network.
*   **Custom Reward Shaping:** Extensive, fine-grained reward structure to guide agent behavior.
*   **`til_environment.gridworld`:** The custom game environment (from the `til-25-environment` submodule), adhering to a PettingZoo-like API for multi-agent interactions.

### ✨ Our Solution & Key Achievements
Our approach to the RL challenge was highly iterative, as reflected in the various experimental "Attempts" in our dedicated RL repository (e.g., "Attempt 1 MARL PPO", "Attempt 2 MLP No Reward", "Attempt 5 CNN DQN"). This process of experimentation with different architectures (MLP, CNN concepts), reward structures, and training methodologies culminated in our final model, "Attempt 10 CNN DQN With Checkpoints."

The core of this final agent was a Deep Q-Network (DQN) with the following key characteristics:

*   **Model Architecture:** The DQN employed a Multi-Layer Perceptron (MLP) with multiple hidden layers (256 units each) to approximate Q-values. While named "CNN DQN" in our attempts (referring to the visual nature of the input), the network processed a 288-dimensional feature vector carefully engineered from the game state, rather than raw pixels. This feature vector included:
    *   A processed representation of the agent's 7x5 viewcone (8 features per tile).
    *   One-hot encoded agent direction.
    *   Normalized agent location (x, y coordinates).
    *   A binary indicator for the Scout role.
    *   The normalized current step count within the episode.
*   **Advanced Training Techniques:**
    *   **Prioritized Experience Replay (PER):** We implemented PER (using a SumTree) to enable the agent to learn more efficiently by focusing on surprising or significant experiences.
    *   **Double DQN:** To reduce overestimation of Q-values and improve stability, the target Q-values were calculated using the policy network to select the best next action and the target network to evaluate that action.
    *   **Epsilon-Greedy Exploration:** An epsilon-greedy strategy was used for action selection during training, with epsilon decaying from an initial value of 0.25 down to 0.01.
    *   **Resumable Training with Checkpoints:** The agent was designed to save and load model checkpoints (`.pth` files). This was crucial for enabling long-duration training sessions that could be resumed, facilitating continuous improvement throughout the hackathon (as seen in "Attempt 9 CNN DQN Resume Training" and "Attempt 10 CNN DQN With Checkpoints").
*   **Customized Reward System (`CUSTOM_REWARDS_DICT`):** A detailed reward structure was critical for guiding agent behavior. This included:
    *   Significant positive rewards for achieving Scout objectives (mission completion, recon, survival).
    *   Large negative penalties for critical failures (Scout captured, wall collisions, agent collisions).
    *   Small penalties per step (time penalty) and for stationary behavior to encourage efficient movement. This tailored reward system (explored in attempts like "Attempt 6 Agent 03 Reward") allowed for differentiated training signals for agents playing as Scout or Guard.
*   **Iterative Refinement:** The final model was the product of extensive experimentation. Early attempts explored concepts like MARL PPO ("Attempt 1") and simpler MLP/DQN versions ("Attempt 2", "Attempt 4", "Attempt 5"). We also experimented with different agent training strategies ("Attempt 7 CNN DQN Train All Agents Simultaneously", "Attempt 8 CNN DQN Train Seperate Agents") and reward designs ("Attempt 3 Guard DQNV2") before arriving at the configuration in "Attempt 10". This iterative process allowed us to fine-tune hyperparameters (learning rate, buffer size, update frequencies) and the overall agent design for optimal performance.

## 🖼️ Computer Vision (CV) Challenge
### 📝 Description
The task was to detect, locate, and classify objects (vehicles, vessels, aircraft) within complex image frames. Our target was 18 specific classes.

### 🔗 Our CV Repository Link
[TimSeah/til-25-data-chefs-CV](https://github.com/TimSeah/til-25-data-chefs-CV) – Our CV model development, including Jupyter notebooks and training scripts, is detailed here.

### 💻 Key Technologies We Used
*   **Python:** Our primary language for scripting and model training.
*   **Jupyter Notebooks:** Extensively used for experimentation, data preprocessing, and visualization.
*   **Ultralytics YOLOv8:** This was the core framework for our object detection model. We specifically used the `yolov8n-p2.yaml` architecture.
*   **PyTorch:** The backend deep learning library for YOLOv8.
*   **OpenCV (`opencv-python`):** For image loading, preprocessing, and augmentation.
*   **YAML:** For dataset and model configuration files.
*   **NumPy:** For numerical operations, especially with image data and bounding boxes.
*   **PowerShell & Shell Scripts:** For automating dataset preparation, training runs, and file management.
*   **Matplotlib & Pillow:** For image visualization and manipulation during development.

### ✨ Our Solution & Key Achievements
For the CV challenge, we focused on training a robust YOLOv8 model to detect 18 distinct classes. Our development process involved several key stages:

1.  **Dataset Preparation:** We organized our image data into `train/` and `val/` sets, with corresponding label files in YOLO format. We created a `yolo_18_classes_config.yaml` file to define the dataset paths and class names:
    `cargo_aircraft, commercial_aircraft, drone, fighter_jet, fighter_plane, helicopter, light aircraft, missile, truck, car, tank, bus, van, cargo_ship, yacht, cruise_ship, warship, sailboat`.
2.  **Model Selection & Configuration:** We chose the `yolov8n-p2.yaml` variant of YOLOv8, aiming for a good balance between performance and speed suitable for the P2 (presumably a specific hardware target or performance tier) requirements. Our input image size was set to 640x640.
3.  **Hyperparameter Tuning:** We utilized YOLOv8's built-in tuning capabilities (`model.tune()`) to find optimal hyperparameters for our specific dataset over 200 iterations, focusing on maximizing mAP. The tuned hyperparameters (learning rate, momentum, augmentation settings like `hsv_h`, `fliplr`, `mosaic`, etc.) were then used for the final training.
4.  **Training Pipeline:**
    *   Our `train_yolo_model.py` script managed the training process.
    *   We implemented custom callbacks during training to monitor progress, estimate remaining time, and periodically save backup model weights (e.g., every 20 epochs). This was crucial for long training runs (we trained for 300 epochs).
    *   We explored a `YOLOWeightedDataset` class (though it might have been experimental or conditionally used) to potentially address class imbalance by adjusting sampling probabilities based on class instance counts.
5.  **Validation:** We used the `model.val()` function to evaluate our trained models on the validation set, focusing on the mAP50-95 metric.
6.  **Iterative Development:** The structure of our CV repository, with a high percentage of Jupyter Notebook usage, indicates an iterative approach involving data exploration, augmentation experiments, and multiple training runs to achieve our final model.

Our final model was the `best.pt` checkpoint from our training run using the tuned hyperparameters, which we then used for submission.

## 🎤 Automatic Speech Recognition (ASR) Challenge
### 📝 Description
We needed to convert spoken audio recordings into accurate text transcripts.

### 🔗 Our ASR Repository Link
[TimSeah/til-25-data-chefs-ASR](https://github.com/TimSeah/til-25-data-chefs-ASR) – Our ASR model development, including fine-tuning scripts and experiments, is detailed here.

### 💻 Key Technologies We Used
*   **Python:** Our primary language for scripting and model operations.
*   **Hugging Face `transformers`:** We heavily relied on this library, specifically using:
    *   `WhisperProcessor` and `WhisperForConditionalGeneration` for our core Whisper model.
    *   `BitsAndBytesConfig` for 4-bit quantization (NF4, float16 compute dtype, double quantization).
*   **PyTorch:** As the backend deep learning framework, enabling GPU acceleration, quantization, and model compilation.
    *   `torch.compile(mode="max-autotune")`: We used this for optimizing model inference speed.
*   **Librosa:** For robust audio resampling to the required 16kHz.
*   **SoundFile:** For reading audio data from bytes.
*   **NumPy:** For numerical operations on audio data.
*   **Jupyter Notebooks, PowerShell & Shell Scripts:** For experimentation, automation, and managing our workflows throughout the ASR development lifecycle.

### ✨ Our Solution & Key Achievements
For the ASR challenge, our strategy revolved around deploying a fine-tuned and optimized OpenAI Whisper model. The `asr_manager.py` script encapsulates our final inference pipeline.

*   **Fine-tuned Whisper Model:** Our core solution utilized a Whisper model that we fine-tuned on the provided dataset (or a relevant custom dataset). The manager script is designed to load a specific fine-tuned version, such as `asr_finetuned_v3_epochs7_cosine_compile`.
*   **Model Loading & Optimization:**
    *   **Dynamic Model Path:** We implemented a robust model loading mechanism that checks for the `MODEL_DIR` environment variable and falls back to predefined relative paths to locate the fine-tuned model files.
    *   **4-bit Quantization:** To enhance efficiency and reduce resource consumption, we loaded the `WhisperForConditionalGeneration` model using 4-bit quantization via `BitsAndBytesConfig`. This included using NF4 quantization, float16 compute data type, and double quantization.
    *   **Inference Compilation:** We applied `torch.compile(self.model, mode="max-autotune")` to the model. This ahead-of-time compilation aims to maximize inference speed by optimizing the model graph, which was particularly beneficial given the competition's speed requirements.
*   **Audio Preprocessing in `ASRManager`:** The `asr` method within our manager handles crucial preprocessing steps directly:
    1.  Loading audio from bytes using `soundfile`.
    2.  Converting stereo audio to mono by averaging channels using `numpy`.
    3.  Resampling the audio to 16kHz using `librosa` to match Whisper's expected input.
*   **Inference Strategy:**
    *   We configured the model generation to use a greedy decoding approach (`num_beams=1`, `do_sample=False`). This provides deterministic and generally the most probable transcription.
    *   The generation config for language and task was explicitly set (defaulting to "english" and "transcribe") to ensure consistent behavior.
    *   The model was set to `eval()` mode for inference.
*   **Robustness:** The `ASRManager` includes error handling for audio decoding, resampling, and feature processing to gracefully manage potential issues with input data.

While earlier explorations (as noted in previous script examples) might have involved other models like Wav2Vec2 or different preprocessing tools like Pydub for dataset preparation, our final, competition-ready `asr_manager.py` showcases a streamlined pipeline focused on an optimized, fine-tuned Whisper model.

## 🧩 Surprise Task: Document Reassembly
### 📝 Description
The Surprise Task challenged us to develop a system for reassembling shredded documents. Given vertical slices of a document (as base64 encoded JPEGs), our system needed to output the correct permutation of these slices to reconstruct the original document. All strips were guaranteed to be of the same width and height and right-side up.

### 🔗 Our Surprise Task Repository Link
[TimSeah/til-25-data-chefs-surprise](https://github.com/TimSeah/til-25-data-chefs-surprise) – Our development for this challenge is located here.

### 💻 Key Technologies We Used
*   **Python:** The primary language for our solution.
*   **Pillow (PIL):** We used Pillow for image manipulation, specifically to open the base64 decoded image slices, convert them to RGB, access pixel data, and determine image dimensions.
*   **NumPy (Implicitly via Pillow/Pixel Math):** While not explicitly imported in the `SurpriseManager`, numerical operations are fundamental to pixel comparisons.
*   **Standard Python Libraries:** `io` for handling byte streams of images, `base64` (implicitly needed to decode input slices, though not shown in `SurpriseManager` which expects bytes).

### ✨ Our Solution & Key Achievements
Our approach to the document reassembly task, implemented in `SurpriseManager`, was based on comparing the pixel similarity of adjacent edges of the document slices.

1.  **Edge Extraction:**
    *   For each input slice (received as bytes), we decoded it and opened it using Pillow.
    *   We extracted the pixel data (RGB tuples) for the leftmost and rightmost columns of each slice. This was handled by our `_get_edges_from_image_bytes` method.
    *   We ensured all slices had a consistent height, as guaranteed by the problem statement.

2.  **Dissimilarity Calculation:**
    *   We calculated a "dissimilarity score" between the right edge of one slice and the left edge of another.
    *   This score was the Sum of Absolute Differences (SAD) of the RGB pixel values along the two edges, implemented in `_calculate_edge_difference`. A lower SAD indicates a better match.
    *   We constructed a dissimilarity matrix `D[i][j]` storing the cost of placing slice `j` immediately to the right of slice `i`.

3.  **Greedy Permutation Search:**
    *   To find the best reassembly order, we employed a greedy algorithm.
    *   We iterated through each slice, considering it as a potential starting (leftmost) piece of the document.
    *   From a chosen starting slice, we iteratively added the next slice that had the lowest dissimilarity (best match) between its left edge and the right edge of the last slice in the current chain.
    *   This process continued until all slices were placed in the permutation.
    *   We kept track of the permutation that resulted in the lowest cumulative dissimilarity across all its connections.

4.  **Input/Output Handling:**
    *   Our `surprise` method in `SurpriseManager` took a list of byte arrays (slices for one document).
    *   It returned a list of integers representing the predicted permutation of the original slice indices.
    *   The overall system (not shown in `SurpriseManager` but part of `surprise_server.py`) would handle the JSON input/output format specified by the challenge, processing multiple document instances.

This edge-matching heuristic allowed us to reconstruct the documents by finding the most visually continuous transitions between slices. The greedy approach provided a computationally feasible way to search for a good permutation, especially given the time constraints of a hackathon challenge.

## 🎉 Final Words from Data Chefs
As first-year TIL participants, we came into the event with minimal prior knowledge about how we could go about tackling these challenges. However, we persevered, and the Google Drive resources and videos helped immensely with the initial learning curve. Namely, for RL, we moved on from using MARL PPO in the example to CNN DQN. Moreover, seeing the technical showcase during the finals day at Marina Bay Sands kindled a spark in us to dive deeper into the field of AI and try out for next year's challenge. All in all, despite not placing in the finals bracket, the journey through the competition was filled with valuable knowledge and experience, and we are glad to have put our best effort into this year's TIL-AI 25 challenge.

A big thank you to DSTA and the TIL-AI organisers!
